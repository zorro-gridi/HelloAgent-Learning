"""
æ™ºèƒ½æœç´¢åŠ©æ‰‹ - åŸºäº LangGraph + Tavily API çš„çœŸå®æœç´¢ç³»ç»Ÿ
1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. ä½¿ç”¨Tavily APIçœŸå®æœç´¢ä¿¡æ¯
3. ç”ŸæˆåŸºäºæœç´¢ç»“æœçš„å›ç­”
"""

import asyncio
from typing import TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver
import os
from dotenv import load_dotenv
from tavily import TavilyClient

import yaml
import re
from pathlib import Path
current_dir = Path(__file__).parent

# è¯»å–é…ç½®æ–‡ä»¶
with open(current_dir.parent.parent.parent / 'config.yaml', 'r') as f:
    config = yaml.safe_load(f)

Provider = 'ModelScope'

# å®šä¹‰çŠ¶æ€ç»“æ„
class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str        # ç”¨æˆ·æŸ¥è¯¢
    search_query: str      # ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢
    search_results: str    # Tavilyæœç´¢ç»“æœ
    final_answer: str      # æœ€ç»ˆç­”æ¡ˆ
    step: str             # å½“å‰æ­¥éª¤

# åˆå§‹åŒ–æ¨¡å‹å’ŒTavilyå®¢æˆ·ç«¯
llm = ChatOpenAI(
    model=config[Provider]['MODEL_NAME'],
    api_key=config[Provider]['API_KEY'],
    base_url=config[Provider]['BASE_URL'],
    temperature=0.7,
    streaming=True,
)

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=config['TAVILY']['API_KEY'])

def understand_query_node(state: SearchState) -> SearchState:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯"""

    # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break

    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"

è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    response = llm.invoke([SystemMessage(content=understand_prompt)])

    # æå–æœç´¢å…³é”®è¯
    response_text = response.content
    search_query = user_message  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢

    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()
    elif "æœç´¢å…³é”®è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢å…³é”®è¯ï¼š")[1].strip()

    return {
        "user_query": response.content,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘ç†è§£æ‚¨çš„éœ€æ±‚ï¼š{response.content}")]
    }

def tavily_search_node(state: SearchState) -> SearchState:
    """æ­¥éª¤2ï¼šä½¿ç”¨Tavily APIè¿›è¡ŒçœŸå®æœç´¢"""

    search_query = state["search_query"]

    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")

        # è°ƒç”¨Tavilyæœç´¢API
        response = tavily_client.search(
            query=search_query,
            search_depth="basic",
            include_answer=True,
            include_raw_content=False,
            max_results=5
        )

        # å¤„ç†æœç´¢ç»“æœ
        search_results = ""

        # ä¼˜å…ˆä½¿ç”¨Tavilyçš„ç»¼åˆç­”æ¡ˆ
        if response.get("answer"):
            search_results = f"ç»¼åˆç­”æ¡ˆï¼š\n{response['answer']}\n\n"

        # æ·»åŠ å…·ä½“çš„æœç´¢ç»“æœ
        if response.get("results"):
            search_results += "ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, result in enumerate(response["results"][:3], 1):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                search_results += f"{i}. {title}\n{content}\næ¥æºï¼š{url}\n\n"

        if not search_results:
            search_results = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content=f"âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œæ­£åœ¨ä¸ºæ‚¨æ•´ç†ç­”æ¡ˆ...")]
        }

    except Exception as e:
        error_msg = f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")

        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{error_msg}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜ï¼Œæˆ‘å°†åŸºäºå·²æœ‰çŸ¥è¯†ä¸ºæ‚¨å›ç­”")]
        }

def generate_answer_node(state: SearchState) -> SearchState:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""

    # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
    if state["step"] == "search_failed":
        # å¦‚æœæœç´¢å¤±è´¥ï¼ŒåŸºäºLLMçŸ¥è¯†å›ç­”
        fallback_prompt = f"""æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨çš„å›ç­”ï¼Œå¹¶è¯´æ˜è¿™æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ã€‚"""

        response = llm.invoke([SystemMessage(content=fallback_prompt)])

        return {
            "final_answer": response.content,
            "step": "completed",
            "messages": [AIMessage(content=response.content)]
        }

    # åŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
    answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æœç´¢ç»“æœï¼š
{state['search_results']}

è¯·è¦æ±‚ï¼š
1. ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”
2. å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–ä»£ç 
3. å¼•ç”¨é‡è¦ä¿¡æ¯çš„æ¥æº
4. å›ç­”è¦ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£
5. å¦‚æœæœç´¢ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œè¯·è¯´æ˜å¹¶æä¾›è¡¥å……å»ºè®®"""

    response = llm.invoke([SystemMessage(content=answer_prompt)])

    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

# æ„å»ºæœç´¢å·¥ä½œæµ
def create_search_assistant():
    workflow = StateGraph(SearchState)

    # æ·»åŠ ä¸‰ä¸ªèŠ‚ç‚¹
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)

    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)

    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)

    return app

async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ™ºèƒ½æœç´¢åŠ©æ‰‹"""

    app = create_search_assistant()

    print("ğŸ” æ™ºèƒ½æœç´¢åŠ©æ‰‹å¯åŠ¨ï¼")
    print("æˆ‘ä¼šä½¿ç”¨Tavily APIä¸ºæ‚¨æœç´¢æœ€æ–°ã€æœ€å‡†ç¡®çš„ä¿¡æ¯")
    print("æ”¯æŒå„ç§é—®é¢˜ï¼šæ–°é—»ã€æŠ€æœ¯ã€çŸ¥è¯†é—®ç­”ç­‰")
    print("(è¾“å…¥ 'quit' é€€å‡º)\n")

    session_count = 0

    while True:
        user_input = input("ğŸ¤” æ‚¨æƒ³äº†è§£ä»€ä¹ˆ: ").strip()

        if user_input.lower() in ['quit', 'q', 'é€€å‡º', 'exit']:
            print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
            break

        if not user_input:
            continue

        session_count += 1
        config = {"configurable": {"thread_id": f"search-session-{session_count}"}}

        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": "",
            "search_query": "",
            "search_results": "",
            "final_answer": "",
            "step": "start"
        }

        try:
            print("\n" + "="*60)

            # æ‰§è¡Œå·¥ä½œæµ
            async for output in app.astream(initial_state, config=config):
                for node_name, node_output in output.items():
                    if "messages" in node_output and node_output["messages"]:
                        latest_message = node_output["messages"][-1]
                        if isinstance(latest_message, AIMessage):
                            if node_name == "understand":
                                print(f"ğŸ§  ç†è§£é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "search":
                                print(f"ğŸ” æœç´¢é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "answer":
                                print(f"\nğŸ’¡ æœ€ç»ˆå›ç­”:\n{latest_message.content}")

            print("\n" + "="*60 + "\n")

        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚\n")

if __name__ == "__main__":
    asyncio.run(main())