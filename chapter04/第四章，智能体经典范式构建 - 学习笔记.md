# 第四章 智能体经典范式构建 - 学习笔记

本章是实践部分的开端，核心目标是**亲手构建智能体的三种经典范式**：ReAct、Plan-and-Solve 和 Reflection。通过从零实现，深入理解其设计原理、工作流程、优缺点及适用场景，从而具备定制和创造智能体的能力，而非仅仅使用现有框架。

### 核心价值

- **理解原理**：超越框架使用，深入理解智能体设计机制。
- **应对工程挑战**：亲手处理输出解析、错误处理、循环控制等问题，培养系统设计能力。
- **创造能力**：掌握原理后，具备定制和从零构建满足复杂需求的全新智能体的能力。

## 4.1 环境准备与基础工具定义

略

## 4.2 ReAct (Reasoning and Acting)

### 核心思想

- **协同循环**：将推理 (Reasoning) 与行动 (Acting) 结合，形成 "Thought -> Action -> Observation" 的循环。
- **动态调整**：根据上一步的观察结果决定下一步的思考和行动，边想边做。

### 工作流程

1. **Thought**：分析当前状况，规划下一步。
2. **Action**：调用工具，格式为 `ToolName[Input]`或 `Finish[Answer]`。
3. **Observation**：工具执行后的返回结果。
4. 将新的 (Action, Observation) 加入历史，循环直至任务完成。

* **形式化表达**：在时间步 *t*，模型 *π*根据问题 *q*和历史轨迹生成当前思考 **$th_t$**和行动 *$a_t$*，环境执行 *a**t*后返回观察 *$o_t$*。

![ReAct交互范式](/Users/zorro/Documents/成长笔记/Agent开发/HelloAgent-Learning/chapter04/image/ReAct交互范式.jpg)

### 实战实现

1. **工具定义**：实现了基于 SerpApi 的 `Search`工具，包含名称、描述和执行逻辑。工具能智能解析搜索结果，优先返回直接答案。

2. **工具执行器**：创建 `ToolExecutor`类，统一管理工具的注册、获取和描述。

3. **智能体构建**：创建 `ReActAgent`类。

   **提示词设计**：模板中包含角色定义、可用工具列表、严格的输出格式规约（Thought/Action）和动态上下文（问题/历史）。

   **核心循环**：循环执行（格式化提示词 -> 调用LLM -> 解析输出 -> 执行动作 -> 整合结果），并设有最大步数限制防止死循环。

   **输出解析**：使用正则表达式从LLM响应中提取 Thought 和 Action。

### 特点与局限性

- **特点**：
  1. **高可解释性**：ReAct 最大的优点之一就是透明。通过 `Thought` 链，我们可以清晰地看到智能体每一步的“心路历程”——它为什么会选择这个工具，下一步又打算做什么。这对于理解、信任和调试智能体的行为至关重要。
  2. **动态规划与纠错能力**：与一次性生成完整计划的范式不同，ReAct 是“走一步，看一步”。它根据每一步从外部世界获得的 `Observation` 来动态调整后续的 `Thought` 和 `Action`。如果上一步的搜索结果不理想，它可以在下一步中修正搜索词，重新尝试。
  3. **工具协同能力**：ReAct 范式天然地将大语言模型的推理能力与外部工具的执行能力结合起来。LLM 负责运筹帷幄（规划和推理），工具负责解决具体问题（搜索、计算），二者协同工作，突破了单一 LLM 在知识时效性、计算准确性等方面的固有局限。
- **局限性**：
  1. **对LLM自身能力的强依赖**：ReAct 流程的成功与否，高度依赖于底层 LLM 的综合能力。如果 LLM 的逻辑推理能力、指令遵循能力或格式化输出能力不足，就很容易在 `Thought` 环节产生错误的规划，或者在 `Action` 环节生成不符合格式的指令，导致整个流程中断。
  2. **执行效率问题**：由于其循序渐进的特性，完成一个任务通常需要多次调用 LLM。每一次调用都伴随着网络延迟和计算成本。对于需要很多步骤的复杂任务，这种串行的“思考-行动”循环可能会导致较高的总耗时和费用。
  3. **提示词的脆弱性**：整个机制的稳定运行建立在一个精心设计的提示词模板之上。模板中的任何微小变动，甚至是用词的差异，都可能影响 LLM 的行为。此外，并非所有模型都能持续稳定地遵循预设的格式，这增加了在实际应用中的不确定性。
  4. **可能陷入局部最优**：步进式的决策模式意味着智能体缺乏一个全局的、长远的规划。它可能会因为眼前的 `Observation` 而选择一个看似正确但长远来看并非最优的路径，甚至在某些情况下陷入“原地打转”的循环中。

### 调试技巧

- 检查完整提示词、分析LLM原始输出、验证工具输入/输出、在提示词中加入 Few-shot 示例、尝试不同模型或参数。

## 4.3 Plan-and-Solve

### 核心思想

- **两阶段解耦**：先制定完整计划 (Plan)，再严格执行 (Solve)，"三思而后行"。其核心动机是为了解决思维链在处理多步骤、复杂问题时容易“偏离轨道”的问题。
- **目标一致**：通过事前规划，避免在复杂多步任务中迷失方向。
- **形式化表达**：
  1. 规划模型 $π_{plan}$ 根据问题 *q* 生成计划 *P*。
  2. 执行模型 $π_{solve}$ 根据 *q*, P 和之前步骤的结果 $s_1$,…,$s_{i−1}$生成第 i 步的解决方案 $s_i$。

![image-20251112095455708](/Users/zorro/Documents/成长笔记/Agent开发/HelloAgent-Learning/chapter04/image/Plan&Solve范式.png)

### 工作流程

1. **规划阶段**：LLM 将复杂问题分解为结构化的步骤列表。
2. **执行阶段**：按顺序执行计划中的每一步，并将上一步的结果作为下一步的输入。

### 实战实现

1. **规划器 (Planner)**：

   * **提示词**：要求模型输出严格的 Python 列表格式的计划，便于解析。使用 `ast.literal_eval`安全地将模型输出字符串转换为列表。

   * **提示词要点**

     - [ ] **角色设定**： “顶级的AI规划专家”，激发模型的专业能力。

     - [ ] **任务描述**： 清晰地定义了“分解问题”的目标。

     - [ ] **格式约束**： 强制要求输出为一个 Python 列表格式的字符串，这极大地简化了后续代码的解析工作，使其比解析自然语言更稳定、更可靠。

2. **执行器 (Executor)**：

   - **提示词**：包含原始问题、完整计划、历史步骤与结果、当前步骤，要求模型仅输出当前步骤的答案。

   * **状态管理**：维护历史记录，确保信息在步骤间正确传递。
   * **提示词要点：**
     * [ ] **原始问题**： 确保模型始终了解最终目标。
     * [ ] **完整计划**： 让模型了解当前步骤在整个任务中的位置。
     * [ ] **历史步骤与结果**： 提供至今为止已经完成的工作，作为当前步骤的直接输入。
     * [ ] **当前步骤**： 明确指示模型现在需要解决哪一个具体任务。

3. **智能体整合**：`PlanAndSolveAgent`类协调规划器和执行器，按顺序调用。

### 适用场景

- 结构性强、可清晰分解的任务，如多步数学题、报告撰写、代码生成。

## 4.4 Reflection (反思)

### 核心思想

- **迭代优化**：引入事后自我校正循环：**"执行 (Execution) -> 反思 (Reflection) -> 优化 (Refinement)"**。
- **自我批判**：通过独立的"评审员"角色对初始输出进行批判性分析，生成反馈。

- **形式化表达**：

  1. 反思模型 $π_{reflect}$ 根据任务和当前输出 $O_i$ 生成反馈 $F_i$。

  2. 优化模型 $π_{refine}$根据任务、$O_i$和 $F_i$生成优化后的输出 $O_{i+1}$。

![image-20251112100942546](/Users/zorro/Documents/成长笔记/Agent开发/HelloAgent-Learning/chapter04/image/Relection范式-1.png)

![image-20251112101020463](/Users/zorro/Documents/成长笔记/Agent开发/HelloAgent-Learning/chapter04/image/:Users:zorro:Library:Application Support:typora-user-images:image-Relection范式-2.png)

### 工作流程

1. **执行 (Execution)**：首先，智能体使用我们熟悉的方法（如 ReAct 或 Plan-and-Solve）尝试完成任务，生成一个初步的解决方案或行动轨迹。这可以看作是“初稿”。

2. **反思 (Reflection)**：

   接着，智能体进入反思阶段。**它会调用一个独立的、或者带有特殊提示词的大语言模型实例**，来扮演一个“评审员”的角色。这个“评审员”会审视第一步生成的“初稿”，并从多个维度进行评估，例如：

   - **事实性错误**：是否存在与常识或已知事实相悖的内容？
   - **逻辑漏洞**：推理过程是否存在不连贯或矛盾之处？
   - **效率问题**：是否有更直接、更简洁的路径来完成任务？
   - **遗漏信息**：是否忽略了问题的某些关键约束或方面？ 根据评估，它会生成一段结构化的**反馈 (Feedback)**，指出具体的问题所在和改进建议，**有效的“批判”是优化的前提**。

3. **优化 (Refinement)**：**迭代式改进**。最后，智能体将“初稿”和“反馈”作为新的上下文，再次调用大语言模型，要求它根据反馈内容对初稿进行修正，生成一个更完善的“修订稿”。

4. **循环进行**，**收敛与终止**。直至满足终止条件（如反馈认为"无需改进"或达到最大迭代次数）。

### 实战实现：代码优化案例

- **任务**：编写一个找出 1 到 n 之间所有素数的 Python 函数。
- **记忆模块 (Memory)**：设计 `Memory`类存储"执行-反思"轨迹，为优化提供上下文。
- **多角色提示词**：
  1. **初始执行提示词**：直接要求生成代码。
  2. **反思提示词**：设定"严格的代码评审专家"角色，专注于分析算法效率瓶颈。
  3. **优化提示词**：要求根据反馈生成优化后的代码。
- **智能体整合**：`ReflectionAgent`类管理迭代循环和记忆。

### 成本收益分析

- **成本**：模型调用开销显著增加、任务延迟提高、提示词工程更复杂。
- **收益**：解决方案质量显著提升（从功能正确到性能高效/逻辑严谨）、鲁棒性和可靠性增强。
- **适用场景**：对结果质量、准确性要求极高，实时性要求不高的任务（如关键代码生成、技术报告、复杂决策）。

## 4.5 本章小结

### 三种范式对比与选择策略

| 范式               | 核心策略           | 优势                     | 典型适用场景                       |
| :----------------- | :----------------- | :----------------------- | :--------------------------------- |
| **ReAct**          | 动态规划，边想边做 | 环境适应性强，能动态纠错 | 需要外部工具输入的探索性任务       |
| **Plan-and-Solve** | 先规划，后执行     | 结构清晰，目标一致性强   | 逻辑路径确定、内部推理密集的任务   |
| **Reflection**     | 执行后反思迭代     | 能显著提升解决方案质量   | 对结果准确性和可靠性要求极高的任务 |

![Summary](/Users/zorro/Documents/成长笔记/Agent开发/HelloAgent-Learning/chapter04/image/Summary.png)