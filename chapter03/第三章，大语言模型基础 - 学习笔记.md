# 第三章 大语言模型基础 - 学习笔记

## 3.1 语言模型与 Transformer 架构

### 3.1.1 从 N-gram 到 RNN

- **语言模型 (LM) 核心任务**：计算一个词序列（句子）出现的概率，用于判断句子的通顺度和自然性。

- **统计语言模型 (N-gram)**：

  **核心思想**：基于马尔可夫假设，即一个词的出现概率只依赖于它前面有限的 `n-1`个词。

  **概率计算**：使用最大似然估计 (MLE)，通过计数来估算概率，例如，当 n = 2 时，Bigram: `P(wi|wi-1) = Count(wi-1, wi) / Count(wi-1)`。

  **根本缺陷**：**数据稀疏性**：未出现的词序列概率为0。

  **泛化能力差**：无法理解词语间的语义相似性。

- **神经网络语言模型**：

  **核心思想**：

  1. **构建一个语义空间**，引入**词嵌入 (Word Embedding)**，将词表示为高维空间中的连续向量，语义相近的词向量距离相近。
  2. **学习从上下文到下一个词的映射**：利用神经网络的强大拟合能力，来学习一个函数。这个函数的输入是前 n−1*n*−1 个词的词向量，输出是词汇表中每个词在当前上下文后出现的概率分布。

  **优势**：通过向量运算（如余弦相似度）能捕捉复杂的语义关系（如类比关系：King - Man + Woman ≈ Queen），解决了N-gram的泛化问题。

  **局限**：上下文窗口固定。

  **循环神经网络 (RNN) 与 LSTM**：

  **RNN**：引入**隐藏状态 (hidden state)** 作为记忆，允许信息在序列中传递，解决了固定窗口问题。

  **问题**：存在**长期依赖问题**（梯度消失/爆炸），难以捕捉长距离依赖。

  **LSTM**：通过**细胞状态 (Cell State)** 和**门控机制**（遗忘门、输入门、输出门）控制信息流，有效缓解了长期依赖问题。

  **问题**：必须按顺序处理数据。第 t 个时间步的计算，必须等待第 t−1 个时间步完成后才能开始。LSTM 无法进行大规模的并行计算，在处理长序列时效率低下，这极大地限制了模型规模和训练速度的提升。

### 3.1.2 Transformer 架构解析

- **提出动机**：解决RNN系列模型无法并行计算、训练效率低下的问题。

- **核心机制**：完全依赖**自注意力 (Self-Attention)** 机制来捕捉序列内依赖关系。

- **Encoder-Decoder 整体结构**（原论文为机器翻译设计）：

  **编码器 (Encoder)**：理解整个输入序列，为每个词元生成富含上下文信息的表示。

  **解码器 (Decoder)**：基于已生成内容和编码器输出，自回归地生成目标序列。

- **关键组件**：**自注意力与多头注意力 (Multi-Head Attention)**：

  1. **Q、K、V向量**：为每个词元生成**查询 (Q)**、**键 (K)**、**值 (V)** 向量。

  2. **计算过程**：`Attention(Q, K, V) = softmax(QK^T / √dk) V`。

  3. **多头机制**：将Q、K、V切分为多份，并行进行多次注意力计算，使模型能关注不同方面的信息，最后合并结果，增强模型表达能力。
     $$
     d_{model} = d_k * H \\
     其中，d_k表示词嵌入的维度，H 为多头的头数
     $$

  4. **逐位置前馈网络 (Position-wise FFN)**：

     * **作用**：对注意力层聚合后的信息进行高阶特征提取。

     * **特点**：独立作用于每个位置的词元，所有位置共享权重。

     * **结构**：两层线性变换夹一个**ReLU激活函数**：`FFN(x) = max(0, xW1 + b1) W2 + b2`。

     * 其中，x 是注意力子层的输出。 W1,b1,W2,b2是可学习的参数。通常，第一个线性层的输出维度 `d_ff` 会远大于输入的维度 `d_model`（例如 `d_ff = 4 * d_model`），经过 ReLU 激活后再通过第二个线性层映射回 `d_model` 维度。这种“先扩大再缩小”的模式，也被称为瓶颈结构，被认为有助于模型学习更丰富的特征表示。

  5. **残差连接 (Add) 与层归一化 (Norm)**：

     **残差连接**：将子层输入直接加到输出上 (`Output = x + Sublayer(x)`)

     本质上 Sublayer 拟合的是上一层拟合的残差，但是加上了 Sublayer 的输入，所以其输出又是指向学习目标的。这种巧妙的设计，缓解了深度网络中的梯度消失问题。

     **层归一化**：对单个样本的所有特征进行归一化，稳定训练，加速收敛。**区别于 BatchNorm**

     表：LayerNorm 对比 BatchNorm

     | 特性                     | BatchNorm（批归一化）                  | LayerNorm（层归一化）   | 为什么对Transformer更重要      |
     | :----------------------- | :------------------------------------- | :---------------------- | :----------------------------- |
     | **归一化维度**           | 跨样本，同一特征                       | 单样本内，所有特征      | 避免填充token和变长序列的影响  |
     | **对Batch Size的敏感性** | 高度敏感，小Batch效果差                | **不敏感**              | 训练更稳定，适应不同Batch Size |
     | **训练 vs. 推理**        | **行为不同**，训练batch>1，推理batch=1 | **行为一致**            | 非常适合自回归生成推理         |
     | **处理变长序列**         | 效果差，受填充影响大                   | **效果优秀**            | NLP任务的刚需                  |
     | **主要应用**             | CV（计算机视觉）                       | **NLP（自然语言处理）** | Transformer是NLP领域的基石模型 |

  6. **位置编码 (Positional Encoding)**：**目的**：为输入序列注入位置信息，弥补自注意力机制本身不包含顺序信息的缺陷。

     **方法**：使用正弦和余弦函数的固定公式计算位置向量，与词嵌入相加。

### 3.1.3 Decoder-Only 架构

- **起源**：由OpenAI的GPT系列模型推广，专注于生成任务。

- **核心思想**：语言的核心任务是预测下一个词。简化结构，只保留Transformer的**解码器**部分。

- **工作模式**：**自回归 (Autoregressive)**。模型根据起始文本，逐个预测后续词元，并将预测结果反馈给自身作为下一步的输入。

- **关键机制**：**掩码自注意力 (Masked Self-Attention)**。确保在预测第t个词时，只能关注到前t-1个词的信息（通过将未来位置的注意力分数掩码为负无穷实现）。

- **优势**：

  **训练目标统一**：预训练任务简单（预测下一个词）。

  **结构简单，易于扩展**：便于构建超大规模模型（如GPT-4, Llama）。

  **天然适合生成任务**：与对话、写作、代码生成等任务完美契合，成为现代大语言模型和智能体的主流架构。

## 3.2 与大语言模型交互

### 3.2.1 提示工程

- **提示 (Prompt)**：与LLM沟通的指令，提示工程旨在设计精准的提示以引导模型产生期望输出。

- **模型采样参数**（调整模型输出的概率分布）：

  1. **Temperature**：控制随机性。`T`小→输出更确定、保守；`T`大→输出更多样、有创意。
  2. **Top-k**：从概率最高的k个候选词中采样。
  3. **Top-p (核采样)**：从累积概率超过p的最小候选词集合中采样。动态适应概率分布。

  * **使用建议**：通常`Top-k`和`Top-p`二选一，`Temperature`可单独或配合使用。`Temperature=0`或`Top-k=1`将使采样退化为贪心搜索（选择最高概率词）。

- **提示类型**：

  1. **零样本提示 (Zero-shot)**：直接给指令，依赖模型泛化能力。

  2. **单样本提示 (One-shot)**：提供一个输入输出示例。

  3. **少样本提示 (Few-shot)**：提供多个示例，让模型更好地理解任务细节。

- **指令调优 (Instruction Tuning)**：使用“指令-回答”数据对预训练模型进行微调，使其能更好地理解和遵循人类指令。现代对话模型（如ChatGPT）都经过指令调优。

- **基础提示技巧**：

  1. **角色扮演 (Role-playing)**：赋予模型特定角色，引导其回答风格和知识范围。
  2. **上下文示例 (In-context Example)**：在提示中提供清晰示例，教会模型处理复杂格式或特定风格的任务。

- **思维链 (Chain-of-Thought, CoT)**：对于复杂问题，能显著提升其推理能力，并使过程更透明。

  实现 CoT 的关键，是在提示中加入一句简单的引导语，如“请逐步思考”或“Let's think step by step”。

### 3.2.2 文本分词

- **分词 (Tokenization)**：将文本转换为模型可处理的数字序列（Token ID）的过程。

- **分词器 (Tokenizer)**：执行分词规则的组件。

- **分词策略**：

  1. **按词分词 (Word-based)**：词表大，存在未登录词 (OOV) 问题。
  2. **按字符分词 (Character-based)**：词表小，无OOV，但学习效率低。
  3. **子词分词 (Subword)**（主流）：平衡词表大小和语义表达。将常见词保留，生僻词拆分为子词。

- **字节对编码 (BPE) 算法**：

  **过程**：从字符级词表开始，迭代地合并语料库中出现频率最高的相邻词元对，直到词表达到预定大小。

  **相关算法**：

  1. WordPiece (BERT, 合并标准是最大化语言模型概率)
  2. SentencePiece (Llama, 将空格视为普通字符，语言无关)。

- **对开发者的意义**：

  1. **上下文窗口**：以Token数计算，需精确管理输入长度。

  2. **API成本**：通常按Token计费。

  3. **模型表现**：分词差异可能导致模型对同一语义的不同表达（如带空格与否、大小写）产生不同理解。

### 3.2.4 模型的选择

- **选型关键考量**：**性能与能力**：参考公开基准测试，不同模型擅长不同任务。
- **成本**：闭源API按Token收费；开源涉及硬件和运维成本。
- **速度 (延迟)**：实时交互应用需选择低延迟模型。
- **上下文窗口**：处理长文档或需长时记忆的任务需大上下文窗口模型。
- **部署方式**：API便捷 vs 本地部署（数据隐私、自主可控）。
- **生态与工具链**：主流模型社区支持、教程、兼容框架更丰富。
- **可微调性与定制化**：开源模型通常更灵活。
- **安全性与伦理**：考虑偏见、毒性、幻觉等问题的缓解措施。

## 3.3 大语言模型的缩放法则与局限性

### 3.3.1 缩放法则

- **缩放法则 (Scaling Laws)**：模型性能与模型参数量 (N)、训练数据量 (D)、计算量 (C) 之间存在可预测的幂律关系。按比例增加三者，性能可预测提升。
- **Chinchilla 定律**：在给定计算预算下，模型参数量与训练数据量存在**最优配比**。为达到最优性能，模型应比过去认为的更小，但需用更多数据训练。强调了数据效率。
- **能力涌现 (Emergent Abilities)**：当模型规模达到一定阈值后，会突然展现出小模型不具备的新能力（如思维链、指令遵循、复杂推理）。表明LLM可能形成了深层次抽象和推理能力。

### 3.3.2 模型幻觉

- **模型幻觉 (Hallucination)**：LLM生成与事实、输入或上下文矛盾或不存在的信息。

- **类型**：

  - **事实性幻觉 (Factual Hallucinations)** ： 模型生成与现实世界事实不符的信息。
  - **忠实性幻觉 (Faithfulness Hallucinations)** ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。
  - **内在幻觉 (Intrinsic Hallucinations)** ： 模型生成的内容与输入信息直接矛盾。

- **产生原因**：训练数据噪声、自回归生成机制（无事实核查）、复杂推理出错。

- **其他局限性**：

  1. **知识时效性**：知识截止于训练数据收集时间。
  2. **偏见**：训练数据中的社会偏见会被模型吸收。

- **缓解方法**：

  1. **数据与模型层面**：高质量数据清洗、RLHF、让模型表达不确定性。

  2. **推理与生成层面**：

     a. **检索增强生成 (RAG)**：从外部知识库检索信息作为生成上下文，有效缓解事实性幻觉。

     b. **多步推理与验证**：引导模型逐步思考并自我检查

     c. **引入外部工具**：调用搜索引擎、计算器等获取实时精确信息。